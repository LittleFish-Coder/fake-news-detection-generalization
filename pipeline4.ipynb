{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 4: Word Embeddings + GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shangyuan191/miniconda3/envs/test_install/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (3988, 2)\n",
      "                                           embedding  label\n",
      "0  [101, 3220, 7158, 5708, 1010, 2040, 2038, 3130...      0\n",
      "1  [101, 11977, 2086, 3283, 1010, 1037, 5637, 215...      0\n",
      "2  [101, 1996, 2388, 999, 1997, 2035, 6550, 2003,...      1\n",
      "3  [101, 1996, 3951, 2162, 2006, 2308, 4247, 1447...      1\n",
      "4  [101, 2004, 4202, 9170, 4455, 2041, 1996, 5223...      0\n",
      "Validation data shape: (998, 2)\n",
      "                                           embedding  label\n",
      "0  [101, 2577, 10805, 18856, 7828, 3240, 1006, 21...      1\n",
      "1  [101, 2079, 2017, 2514, 2009, 1999, 2115, 3093...      0\n",
      "2  [101, 15147, 1996, 2548, 2155, 5935, 2023, 285...      0\n",
      "3  [101, 5074, 9932, 4244, 1010, 2280, 4419, 2739...      0\n",
      "4  [101, 2137, 7642, 2775, 4424, 6905, 2099, 1998...      0\n",
      "Testing data shape: (1247, 2)\n",
      "   id                                          embedding\n",
      "0   2  [101, 1996, 2418, 9458, 3601, 2982, 5103, 2001...\n",
      "1   3  [101, 1996, 4164, 1010, 2112, 1997, 1523, 1996...\n",
      "2   4  [101, 19166, 12791, 10220, 2000, 2831, 2000, 2...\n",
      "3   5  [101, 2023, 2003, 4788, 2084, 1037, 15116, 199...\n",
      "4   6  [101, 11320, 11639, 2139, 2625, 13699, 2015, 2...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv('dataset/train_embedding.csv', sep='\\t', encoding='utf-8')\n",
    "val_df = pd.read_csv('dataset/val_embedding.csv', sep='\\t', encoding='utf-8')\n",
    "test_df = pd.read_csv('dataset/test_embedding.csv', sep='\\t', encoding='utf-8')\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "print(val_df.head())\n",
    "print(f\"Testing data shape: {test_df.shape}\")\n",
    "print(test_df.head())   # no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3220, 7158, 5708, 1010, 2040, 2038, 3130, 2042, 16875, 2055, 2010, 9415, 6905, 1998, 5983, 8761, 1010, 2003, 2085, 3098, 2039, 2055, 2010, 13798, 1012, 1996, 2756, 1011, 2095, 1011, 2214, 2567, 1997, 10457, 13334, 2102, 3337, 2632, 2819, 4172, 5708, 1056, 28394, 3064, 2006, 5095, 2305, 1037, 2146, 2330, 3661, 1999, 2029, 2002, 28049, 2010, 8432, 2000, 2119, 2273, 1998, 2308, 2144, 2002, 2001, 2410, 1012, 1000, 2045, 1521, 1055, 2242, 1045, 1521, 1040, 2066, 2000, 2360, 2008, 1045, 2514, 2003, 2590, 2005, 2870, 1998, 2026, 4767, 2008, 2038, 2042, 15243, 2006, 2026, 3108, 2005, 3053, 2431, 1997, 2026, 2166, 1010, 1000, 2002, 2626, 1012, 1000, 2023, 2987, 1521, 1056, 3288, 2033, 9467, 1010, 2074, 1037, 3635, 1998, 10859, 1045, 2031, 2218, 3031, 2005, 1037, 2146, 2051, 2008, 1045, 2052, 2066, 4196, 2125, 2033, 1012, 1000, 2002, 7607, 1010, 1000, 1045, 3473, 2039, 1999, 2023, 4024, 3068, 2012, 1037, 2200, 2402, 2287, 1998, 2043, 1045, 2001, 2105, 2410, 2086, 2214, 1045, 2318, 2000, 2424, 3337, 1998, 3057, 8702, 1012, 1000, 2002, 2059, 7657, 1010, 1000, 2045, 2020, 2086, 2008, 2253, 2011, 2008, 1045, 2245, 2055, 1010, 2021, 2009, 2347, 1521, 1056, 2127, 1045, 2001, 2459, 2086, 2214, 1010, 2044, 1037, 2261, 6550, 2007, 3057, 1010, 1045, 2018, 2019, 3325, 2007, 1037, 3287, 2008, 1045, 2018, 2019, 8432, 2000, 2040, 1045, 2036, 2499, 2007, 1998, 3473, 2039, 2007, 1012, 1000, 5708, 4247, 1010, 1000, 2000, 2033, 2189, 2038, 2467, 2042, 2026, 3379, 1012, 2189, 2097, 2467, 2022, 2054, 9099, 23865, 2015, 2149, 1998, 2870, 1012, 1996, 2996, 2038, 2467, 2042, 2026, 3647, 4033, 1012, 2021, 1996, 7209, 3125, 2005, 2033, 2003, 2000, 2022, 8510, 1012, 1045, 2196, 2215, 2000, 2022, 1037, 3275, 1997, 10520, 1012, 1000, 1037, 9925, 1010, 3516, 1010, 3128, 1010, 2002, 4515, 1996, 3661, 2007, 1037, 14686, 2013, 3220, 2879, 2577, 1010, 2040, 2036, 2038, 2042, 2330, 1999, 10537, 2010, 13798, 1012, 1000, 1996, 2190, 14686, 2000, 7680, 1005, 1045, 1005, 2310, 2196, 2371, 2004, 2295, 1045, 2134, 1005, 1056, 7141, 1010, 1045, 2074, 6051, 2004, 2295, 1045, 2106, 1012, 1005, 1517, 2879, 2577, 1000, 5708, 1005, 1055, 1056, 28394, 2102, 4076, 2010, 2251, 2321, 6545, 1999, 4108, 2006, 10928, 1997, 4439, 2104, 1996, 3747, 1998, 16204, 6664, 1012, 2010, 6513, 1010, 7063, 6262, 1010, 2001, 1999, 1996, 4628, 2835, 1997, 1996, 2482, 1998, 2001, 2036, 2579, 2046, 2610, 9968, 1012, 5708, 2001, 2207, 2044, 14739, 15358, 2005, 1002, 1018, 1010, 19827, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'str'>\n",
      "[101, 3220, 7158, 5708, 1010, 2040, 2038, 3130, 2042, 16875, 2055, 2010, 9415, 6905, 1998, 5983, 8761, 1010, 2003, 2085, 3098, 2039, 2055, 2010, 13798, 1012, 1996, 2756, 1011, 2095, 1011, 2214, 2567, 1997, 10457, 13334, 2102, 3337, 2632, 2819, 4172, 5708, 1056, 28394, 3064, 2006, 5095, 2305, 1037, 2146, 2330, 3661, 1999, 2029, 2002, 28049, 2010, 8432, 2000, 2119, 2273, 1998, 2308, 2144, 2002, 2001, 2410, 1012, 1000, 2045, 1521, 1055, 2242, 1045, 1521, 1040, 2066, 2000, 2360, 2008, 1045, 2514, 2003, 2590, 2005, 2870, 1998, 2026, 4767, 2008, 2038, 2042, 15243, 2006, 2026, 3108, 2005, 3053, 2431, 1997, 2026, 2166, 1010, 1000, 2002, 2626, 1012, 1000, 2023, 2987, 1521, 1056, 3288, 2033, 9467, 1010, 2074, 1037, 3635, 1998, 10859, 1045, 2031, 2218, 3031, 2005, 1037, 2146, 2051, 2008, 1045, 2052, 2066, 4196, 2125, 2033, 1012, 1000, 2002, 7607, 1010, 1000, 1045, 3473, 2039, 1999, 2023, 4024, 3068, 2012, 1037, 2200, 2402, 2287, 1998, 2043, 1045, 2001, 2105, 2410, 2086, 2214, 1045, 2318, 2000, 2424, 3337, 1998, 3057, 8702, 1012, 1000, 2002, 2059, 7657, 1010, 1000, 2045, 2020, 2086, 2008, 2253, 2011, 2008, 1045, 2245, 2055, 1010, 2021, 2009, 2347, 1521, 1056, 2127, 1045, 2001, 2459, 2086, 2214, 1010, 2044, 1037, 2261, 6550, 2007, 3057, 1010, 1045, 2018, 2019, 3325, 2007, 1037, 3287, 2008, 1045, 2018, 2019, 8432, 2000, 2040, 1045, 2036, 2499, 2007, 1998, 3473, 2039, 2007, 1012, 1000, 5708, 4247, 1010, 1000, 2000, 2033, 2189, 2038, 2467, 2042, 2026, 3379, 1012, 2189, 2097, 2467, 2022, 2054, 9099, 23865, 2015, 2149, 1998, 2870, 1012, 1996, 2996, 2038, 2467, 2042, 2026, 3647, 4033, 1012, 2021, 1996, 7209, 3125, 2005, 2033, 2003, 2000, 2022, 8510, 1012, 1045, 2196, 2215, 2000, 2022, 1037, 3275, 1997, 10520, 1012, 1000, 1037, 9925, 1010, 3516, 1010, 3128, 1010, 2002, 4515, 1996, 3661, 2007, 1037, 14686, 2013, 3220, 2879, 2577, 1010, 2040, 2036, 2038, 2042, 2330, 1999, 10537, 2010, 13798, 1012, 1000, 1996, 2190, 14686, 2000, 7680, 1005, 1045, 1005, 2310, 2196, 2371, 2004, 2295, 1045, 2134, 1005, 1056, 7141, 1010, 1045, 2074, 6051, 2004, 2295, 1045, 2106, 1012, 1005, 1517, 2879, 2577, 1000, 5708, 1005, 1055, 1056, 28394, 2102, 4076, 2010, 2251, 2321, 6545, 1999, 4108, 2006, 10928, 1997, 4439, 2104, 1996, 3747, 1998, 16204, 6664, 1012, 2010, 6513, 1010, 7063, 6262, 1010, 2001, 1999, 1996, 4628, 2835, 1997, 1996, 2482, 1998, 2001, 2036, 2579, 2046, 2610, 9968, 1012, 5708, 2001, 2207, 2044, 14739, 15358, 2005, 1002, 1018, 1010, 19827, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# transfrom embedding to list of int\n",
    "import ast\n",
    "\n",
    "# read\n",
    "print(train_df['embedding'][0])   \n",
    "print(type(train_df['embedding'][0])) # string\n",
    "\n",
    "# convert the embeddings to list\n",
    "train_df['embedding'] = train_df['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "val_df['embedding'] = val_df['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "test_df['embedding'] = test_df['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# convert the embeddings to list of integers\n",
    "train_df['embedding'] = train_df['embedding'].apply(lambda x: list(map(int, x)))\n",
    "val_df['embedding'] = val_df['embedding'].apply(lambda x: list(map(int, x)))\n",
    "test_df['embedding'] = test_df['embedding'].apply(lambda x: list(map(int, x)))\n",
    "\n",
    "print(train_df['embedding'][0])\n",
    "print(type(train_df['embedding'][0])) # list of integers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3220, 7158, 5708, 1010, 2040, 2038, 3130, 2042, 16875, 2055, 2010, 9415, 6905, 1998, 5983, 8761, 1010, 2003, 2085, 3098, 2039, 2055, 2010, 13798, 1012, 1996, 2756, 1011, 2095, 1011, 2214, 2567, 1997, 10457, 13334, 2102, 3337, 2632, 2819, 4172, 5708, 1056, 28394, 3064, 2006, 5095, 2305, 1037, 2146, 2330, 3661, 1999, 2029, 2002, 28049, 2010, 8432, 2000, 2119, 2273, 1998, 2308, 2144, 2002, 2001, 2410, 1012, 1000, 2045, 1521, 1055, 2242, 1045, 1521, 1040, 2066, 2000, 2360, 2008, 1045, 2514, 2003, 2590, 2005, 2870, 1998, 2026, 4767, 2008, 2038, 2042, 15243, 2006, 2026, 3108, 2005, 3053, 2431, 1997, 2026, 2166, 1010, 1000, 2002, 2626, 1012, 1000, 2023, 2987, 1521, 1056, 3288, 2033, 9467, 1010, 2074, 1037, 3635, 1998, 10859, 1045, 2031, 2218, 3031, 2005, 1037, 2146, 2051, 2008, 1045, 2052, 2066, 4196, 2125, 2033, 1012, 1000, 2002, 7607, 1010, 1000, 1045, 3473, 2039, 1999, 2023, 4024, 3068, 2012, 1037, 2200, 2402, 2287, 1998, 2043, 1045, 2001, 2105, 2410, 2086, 2214, 1045, 2318, 2000, 2424, 3337, 1998, 3057, 8702, 1012, 1000, 2002, 2059, 7657, 1010, 1000, 2045, 2020, 2086, 2008, 2253, 2011, 2008, 1045, 2245, 2055, 1010, 2021, 2009, 2347, 1521, 1056, 2127, 1045, 2001, 2459, 2086, 2214, 1010, 2044, 1037, 2261, 6550, 2007, 3057, 1010, 1045, 2018, 2019, 3325, 2007, 1037, 3287, 2008, 1045, 2018, 2019, 8432, 2000, 2040, 1045, 2036, 2499, 2007, 1998, 3473, 2039, 2007, 1012, 1000, 5708, 4247, 1010, 1000, 2000, 2033, 2189, 2038, 2467, 2042, 2026, 3379, 1012, 2189, 2097, 2467, 2022, 2054, 9099, 23865, 2015, 2149, 1998, 2870, 1012, 1996, 2996, 2038, 2467, 2042, 2026, 3647, 4033, 1012, 2021, 1996, 7209, 3125, 2005, 2033, 2003, 2000, 2022, 8510, 1012, 1045, 2196, 2215, 2000, 2022, 1037, 3275, 1997, 10520, 1012, 1000, 1037, 9925, 1010, 3516, 1010, 3128, 1010, 2002, 4515, 1996, 3661, 2007, 1037, 14686, 2013, 3220, 2879, 2577, 1010, 2040, 2036, 2038, 2042, 2330, 1999, 10537, 2010, 13798, 1012, 1000, 1996, 2190, 14686, 2000, 7680, 1005, 1045, 1005, 2310, 2196, 2371, 2004, 2295, 1045, 2134, 1005, 1056, 7141, 1010, 1045, 2074, 6051, 2004, 2295, 1045, 2106, 1012, 1005, 1517, 2879, 2577, 1000, 5708, 1005, 1055, 1056, 28394, 2102, 4076, 2010, 2251, 2321, 6545, 1999, 4108, 2006, 10928, 1997, 4439, 2104, 1996, 3747, 1998, 16204, 6664, 1012, 2010, 6513, 1010, 7063, 6262, 1010, 2001, 1999, 1996, 4628, 2835, 1997, 1996, 2482, 1998, 2001, 2036, 2579, 2046, 2610, 9968, 1012, 5708, 2001, 2207, 2044, 14739, 15358, 2005, 1002, 1018, 1010, 19827, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n",
      "3988\n",
      "[101, 2577, 10805, 18856, 7828, 3240, 1006, 2141, 2089, 1020, 1010, 3777, 1007, 2003, 2019, 2137, 3364, 1010, 2472, 1010, 3135, 1010, 11167, 1010, 1998, 6883, 1012, 2002, 2038, 2363, 2093, 3585, 7595, 2982, 2005, 2010, 2147, 2004, 2019, 3364, 1998, 2048, 2914, 2982, 1010, 2028, 2005, 3772, 1999, 9042, 2050, 1006, 2294, 1007, 1998, 1996, 2060, 2005, 2522, 1011, 5155, 12098, 3995, 1006, 2262, 1007, 1012, 18856, 7828, 3240, 2081, 2010, 3772, 2834, 2006, 2547, 1999, 3301, 1010, 1998, 2101, 4227, 2898, 5038, 1999, 2010, 2535, 2004, 2852, 1012, 8788, 5811, 2006, 1996, 2146, 1011, 2770, 2966, 3689, 9413, 2013, 2807, 2000, 2639, 1010, 2005, 2029, 2002, 2363, 2048, 18474, 10096, 2400, 9930, 1012, 2096, 2551, 2006, 9413, 1010, 2002, 2211, 15411, 1037, 3528, 1997, 2877, 4395, 1999, 3152, 1010, 2164, 1996, 16251, 2143, 8942, 1004, 5863, 1006, 2722, 1007, 1998, 1996, 4126, 4038, 2041, 1997, 4356, 1006, 2687, 1007, 1010, 1999, 2029, 2002, 2034, 2499, 2007, 2472, 7112, 2061, 4063, 4059, 2232, 1010, 2040, 2052, 2468, 1037, 2146, 1011, 2051, 18843, 1012, 1999, 2639, 1010, 2002, 2165, 1996, 2599, 2535, 1999, 2093, 5465, 1010, 1037, 2092, 1011, 2363, 2162, 18312, 2275, 2076, 1996, 6084, 2162, 1012, 1999, 2541, 1010, 18856, 7828, 3240, 1005, 1055, 4476, 8723, 2007, 1996, 2713, 1997, 2010, 5221, 3293, 3112, 1010, 1996, 2002, 2923, 4038, 12661, 4153, 1005, 1055, 5408, 1010, 1996, 2034, 1997, 2054, 2150, 1037, 11544, 4626, 18856, 7828, 3240, 1012, 2002, 2081, 2010, 21635, 2834, 1037, 2095, 2101, 2007, 1996, 16747, 8645, 4038, 21444, 1997, 1037, 4795, 2568, 1010, 1998, 2038, 2144, 2856, 1996, 3439, 3689, 2204, 2305, 1010, 1998, 2204, 6735, 1006, 2384, 1007, 1010, 1996, 2998, 4038, 5898, 13038, 1006, 2263, 1007, 1010, 1996, 2576, 3689, 1996, 8909, 2229, 1997, 2233, 1006, 2249, 1007, 1010, 1998, 1996, 2162, 2143, 1996, 10490, 2273, 1006, 2297, 1007, 1012, 18856, 7828, 3240, 2180, 2019, 2914, 2400, 2005, 2190, 4637, 3364, 2005, 1996, 2690, 2264, 10874, 9042, 2050, 1006, 2384, 1007, 1010, 1998, 3525, 3687, 2190, 3364, 9930, 2005, 1996, 3423, 10874, 2745, 11811, 1006, 2289, 1007, 1998, 1996, 4038, 1011, 16547, 2039, 1999, 1996, 2250, 1006, 2268, 1007, 1998, 1996, 8481, 1006, 2249, 1007, 1012, 1999, 2286, 1010, 2002, 2363, 1996, 2914, 2400, 2005, 2190, 3861, 2005, 5155, 1996, 2576, 10874, 12098, 3995, 1012, 2002, 2003, 1996, 2069, 2711, 2040, 2038, 2042, 4222, 2005, 2914, 2982, 1999, 2416, 2367, 7236, 1012, 1031, 1017, 1033, 1999, 2268, 1010, 18856, 7828, 3240, 2001, 2443, 1999, 2051, 1005, 1055, 3296, 2051, 2531, 2004, 2028, 1997, 1996, 1000, 2087, 6383, 2111, 1999, 1996, 2088, 1000, 1012, 1031, 1018, 1033, 2002, 2003, 2036, 3264, 2005, 2010, 2576, 1998, 3171, 16841, 1010, 1998, 2038, 2366, 2004, 2028, 1997, 1996, 2142, 3741, 28938, 1997, 3521, 2144, 2254, 2861, 1010, 2263, 1012, 1031, 1019, 1033, 1031, 1020, 1033, 1031, 1021, 1033, 2010, 11470, 2147, 2950, 2010, 12288, 1997, 4531, 1037, 5813, 2005, 1996, 18243, 27942, 4736, 1010, 6274, 5029, 2005, 1996, 2230, 12867, 8372, 1010, 7508, 14052, 5038, 1010, 2432, 19267, 1010, 1023, 1013, 2340, 5694, 1010, 1998, 4526, 15693, 2107, 102]\n",
      "<class 'list'>\n",
      "998\n",
      "[101, 1996, 2418, 9458, 3601, 2982, 5103, 2001, 2218, 2006, 2257, 2410, 1010, 2418, 1012, 1031, 1015, 1033, 1996, 2982, 6334, 1996, 2095, 1005, 1055, 10106, 1999, 2189, 1010, 2143, 1010, 2547, 1010, 2998, 1010, 4827, 1010, 4038, 1010, 1998, 1996, 4274, 1010, 1998, 2020, 5444, 2006, 2011, 7193, 2542, 1999, 1996, 3915, 1010, 4793, 2410, 1998, 2058, 2083, 2536, 2591, 2865, 4573, 1012, 1031, 1016, 1033, 1037, 2093, 3178, 3315, 2782, 2170, 1000, 9458, 17037, 1000, 1998, 4354, 2011, 5180, 2703, 2001, 18498, 7580, 2006, 7858, 2007, 2070, 1997, 1996, 2724, 6037, 2076, 1996, 9458, 3601, 3743, 1012, 1031, 1017, 1033, 22222, 1019, 2363, 1996, 7725, 5476, 2400, 1012, 1031, 1018, 1033, 2802, 1996, 2265, 1010, 2195, 12330, 1010, 2164, 13226, 15876, 11818, 3619, 1010, 16729, 10259, 2050, 1998, 10294, 14855, 5397, 25698, 8280, 1996, 10530, 1997, 1996, 2418, 15908, 1996, 2157, 8320, 1998, 6628, 13496, 2000, 3713, 2041, 2114, 4808, 1998, 5223, 1012, 2023, 2003, 1996, 2034, 5103, 2144, 2526, 2000, 2025, 2421, 1037, 3677, 1012, 9567, 1031, 10086, 1033, 25588, 1031, 10086, 1033, 4791, 1998, 17853, 1031, 10086, 1033, 1996, 2034, 4400, 1997, 9930, 2020, 2623, 2006, 2238, 2539, 1010, 2418, 1012, 1031, 1022, 1033, 1996, 2117, 4400, 2001, 2623, 2006, 2251, 2260, 1010, 2418, 1012, 1031, 1023, 1033, 4791, 2024, 3205, 2034, 1010, 1999, 7782, 1012, 1031, 2184, 1033, 5691, 1031, 10086, 1033, 2547, 1031, 10086, 1033, 5691, 1004, 2547, 1031, 10086, 1033, 2189, 1031, 10086, 1033, 3617, 1031, 10086, 1033, 4827, 1031, 10086, 1033, 2998, 1031, 10086, 1033, 25408, 1031, 10086, 1033, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n",
      "1247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_df['embedding'][0])\n",
    "print(type(train_df['embedding'][0])) # list of integers\n",
    "print(len(train_df))\n",
    "\n",
    "print(val_df['embedding'][0])\n",
    "print(type(val_df['embedding'][0])) # list of integers\n",
    "print(len(val_df))\n",
    "\n",
    "print(test_df['embedding'][0])\n",
    "print(type(test_df['embedding'][0])) # list of integers\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "# %pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# %pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# %pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "# %pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['embedding', 'label'], dtype='object')\n",
      "Index(['embedding', 'label'], dtype='object')\n",
      "Index(['id', 'embedding'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "print(train_df.columns)\n",
    "print(val_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建PyTorch Geometric数据对象\n",
    "def create_data(df):\n",
    "    X = torch.tensor(df['embedding'].tolist(), dtype=torch.float32)\n",
    "    Y = torch.tensor(df['label'].tolist(), dtype=torch.long)\n",
    "    \n",
    "    # 假设邻接矩阵是已知的（这里使用全连接图作为示例）\n",
    "    edge_index = torch.tensor([\n",
    "        [0, 1, 2, 1, 0, 2],  # 每个边的起点\n",
    "        [1, 2, 0, 0, 2, 1]   # 每个边的终点\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    return Data(x=X, edge_index=edge_index, y=Y)\n",
    "\n",
    "train_data = create_data(train_df)\n",
    "val_data = create_data(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 模型参数\n",
    "input_dim = train_data.num_node_features\n",
    "hidden_dim = 16\n",
    "output_dim = len(train_df['label'].unique())\n",
    "\n",
    "# 创建模型实例\n",
    "model = GCN(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/2000], Loss: 0.6732\n",
      "Epoch [40/2000], Loss: 0.6732\n",
      "Epoch [60/2000], Loss: 0.6732\n",
      "Epoch [80/2000], Loss: 0.6732\n",
      "Epoch [100/2000], Loss: 0.6732\n",
      "Epoch [120/2000], Loss: 0.6732\n",
      "Epoch [140/2000], Loss: 0.6732\n",
      "Epoch [160/2000], Loss: 0.6732\n",
      "Epoch [180/2000], Loss: 0.6732\n",
      "Epoch [200/2000], Loss: 0.6732\n",
      "Epoch [220/2000], Loss: 0.6732\n",
      "Epoch [240/2000], Loss: 0.6732\n",
      "Epoch [260/2000], Loss: 0.6732\n",
      "Epoch [280/2000], Loss: 0.6732\n",
      "Epoch [300/2000], Loss: 0.6732\n",
      "Epoch [320/2000], Loss: 0.6732\n",
      "Epoch [340/2000], Loss: 0.6732\n",
      "Epoch [360/2000], Loss: 0.6732\n",
      "Epoch [380/2000], Loss: 0.6732\n",
      "Epoch [400/2000], Loss: 0.6732\n",
      "Epoch [420/2000], Loss: 0.6732\n",
      "Epoch [440/2000], Loss: 0.6732\n",
      "Epoch [460/2000], Loss: 0.6732\n",
      "Epoch [480/2000], Loss: 0.6732\n",
      "Epoch [500/2000], Loss: 0.6732\n",
      "Epoch [520/2000], Loss: 0.6732\n",
      "Epoch [540/2000], Loss: 0.6732\n",
      "Epoch [560/2000], Loss: 0.6732\n",
      "Epoch [580/2000], Loss: 0.6732\n",
      "Epoch [600/2000], Loss: 0.6732\n",
      "Epoch [620/2000], Loss: 0.6732\n",
      "Epoch [640/2000], Loss: 0.6732\n",
      "Epoch [660/2000], Loss: 0.6732\n",
      "Epoch [680/2000], Loss: 0.6732\n",
      "Epoch [700/2000], Loss: 0.6732\n",
      "Epoch [720/2000], Loss: 0.6732\n",
      "Epoch [740/2000], Loss: 0.6732\n",
      "Epoch [760/2000], Loss: 0.6732\n",
      "Epoch [780/2000], Loss: 0.6732\n",
      "Epoch [800/2000], Loss: 0.6732\n",
      "Epoch [820/2000], Loss: 0.6732\n",
      "Epoch [840/2000], Loss: 0.6732\n",
      "Epoch [860/2000], Loss: 0.6732\n",
      "Epoch [880/2000], Loss: 0.6732\n",
      "Epoch [900/2000], Loss: 0.6732\n",
      "Epoch [920/2000], Loss: 0.6732\n",
      "Epoch [940/2000], Loss: 0.6732\n",
      "Epoch [960/2000], Loss: 0.6732\n",
      "Epoch [980/2000], Loss: 0.6732\n",
      "Epoch [1000/2000], Loss: 0.6732\n",
      "Epoch [1020/2000], Loss: 0.6732\n",
      "Epoch [1040/2000], Loss: 0.6732\n",
      "Epoch [1060/2000], Loss: 0.6732\n",
      "Epoch [1080/2000], Loss: 0.6732\n",
      "Epoch [1100/2000], Loss: 0.6732\n",
      "Epoch [1120/2000], Loss: 0.6732\n",
      "Epoch [1140/2000], Loss: 0.6732\n",
      "Epoch [1160/2000], Loss: 0.6732\n",
      "Epoch [1180/2000], Loss: 0.6732\n",
      "Epoch [1200/2000], Loss: 0.6732\n",
      "Epoch [1220/2000], Loss: 0.6732\n",
      "Epoch [1240/2000], Loss: 0.6732\n",
      "Epoch [1260/2000], Loss: 0.6732\n",
      "Epoch [1280/2000], Loss: 0.6732\n",
      "Epoch [1300/2000], Loss: 0.6732\n",
      "Epoch [1320/2000], Loss: 0.6732\n",
      "Epoch [1340/2000], Loss: 0.6732\n",
      "Epoch [1360/2000], Loss: 0.6732\n",
      "Epoch [1380/2000], Loss: 0.6732\n",
      "Epoch [1400/2000], Loss: 0.6732\n",
      "Epoch [1420/2000], Loss: 0.6732\n",
      "Epoch [1440/2000], Loss: 0.6732\n",
      "Epoch [1460/2000], Loss: 0.6732\n",
      "Epoch [1480/2000], Loss: 0.6732\n",
      "Epoch [1500/2000], Loss: 0.6732\n",
      "Epoch [1520/2000], Loss: 0.6732\n",
      "Epoch [1540/2000], Loss: 0.6732\n",
      "Epoch [1560/2000], Loss: 0.6732\n",
      "Epoch [1580/2000], Loss: 0.6732\n",
      "Epoch [1600/2000], Loss: 0.6732\n",
      "Epoch [1620/2000], Loss: 0.6732\n",
      "Epoch [1640/2000], Loss: 0.6732\n",
      "Epoch [1660/2000], Loss: 0.6732\n",
      "Epoch [1680/2000], Loss: 0.6732\n",
      "Epoch [1700/2000], Loss: 0.6732\n",
      "Epoch [1720/2000], Loss: 0.6732\n",
      "Epoch [1740/2000], Loss: 0.6732\n",
      "Epoch [1760/2000], Loss: 0.6732\n",
      "Epoch [1780/2000], Loss: 0.6732\n",
      "Epoch [1800/2000], Loss: 0.6732\n",
      "Epoch [1820/2000], Loss: 0.6732\n",
      "Epoch [1840/2000], Loss: 0.6732\n",
      "Epoch [1860/2000], Loss: 0.6732\n",
      "Epoch [1880/2000], Loss: 0.6732\n",
      "Epoch [1900/2000], Loss: 0.6732\n",
      "Epoch [1920/2000], Loss: 0.6732\n",
      "Epoch [1940/2000], Loss: 0.6732\n",
      "Epoch [1960/2000], Loss: 0.6732\n",
      "Epoch [1980/2000], Loss: 0.6732\n",
      "Epoch [2000/2000], Loss: 0.6732\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data)\n",
    "    loss = F.nll_loss(out, train_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train()\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 59.95%\n",
      "Validation Accuracy: 58.22%\n"
     ]
    }
   ],
   "source": [
    "def test(data):\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = pred.eq(data.y).sum().item()\n",
    "    accuracy = correct / len(data.y)\n",
    "    return accuracy\n",
    "\n",
    "train_acc = test(train_data)\n",
    "val_acc = test(val_data)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc * 100:.2f}%')\n",
    "print(f'Validation Accuracy: {val_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
