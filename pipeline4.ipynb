{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 4: Word Embeddings + GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (3988, 2)\n",
      "                                           embedding  label\n",
      "0  [101, 3220, 7158, 5708, 1010, 2040, 2038, 3130...      0\n",
      "1  [101, 11977, 2086, 3283, 1010, 1037, 5637, 215...      0\n",
      "2  [101, 1996, 2388, 999, 1997, 2035, 6550, 2003,...      1\n",
      "3  [101, 1996, 3951, 2162, 2006, 2308, 4247, 1447...      1\n",
      "4  [101, 2004, 4202, 9170, 4455, 2041, 1996, 5223...      0\n",
      "Validation data shape: (998, 2)\n",
      "                                           embedding  label\n",
      "0  [101, 2577, 10805, 18856, 7828, 3240, 1006, 21...      1\n",
      "1  [101, 2079, 2017, 2514, 2009, 1999, 2115, 3093...      0\n",
      "2  [101, 15147, 1996, 2548, 2155, 5935, 2023, 285...      0\n",
      "3  [101, 5074, 9932, 4244, 1010, 2280, 4419, 2739...      0\n",
      "4  [101, 2137, 7642, 2775, 4424, 6905, 2099, 1998...      0\n",
      "Testing data shape: (1247, 2)\n",
      "   id                                          embedding\n",
      "0   2  [101, 1996, 2418, 9458, 3601, 2982, 5103, 2001...\n",
      "1   3  [101, 1996, 4164, 1010, 2112, 1997, 1523, 1996...\n",
      "2   4  [101, 19166, 12791, 10220, 2000, 2831, 2000, 2...\n",
      "3   5  [101, 2023, 2003, 4788, 2084, 1037, 15116, 199...\n",
      "4   6  [101, 11320, 11639, 2139, 2625, 13699, 2015, 2...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv('dataset/train_embedding.csv', sep='\\t', encoding='utf-8')\n",
    "val_df = pd.read_csv('dataset/val_embedding.csv', sep='\\t', encoding='utf-8')\n",
    "test_df = pd.read_csv('dataset/test_embedding.csv', sep='\\t', encoding='utf-8')\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "print(val_df.head())\n",
    "print(f\"Testing data shape: {test_df.shape}\")\n",
    "print(test_df.head())   # no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3220, 7158, 5708, 1010, 2040, 2038, 3130, 2042, 16875, 2055, 2010, 9415, 6905, 1998, 5983, 8761, 1010, 2003, 2085, 3098, 2039, 2055, 2010, 13798, 1012, 1996, 2756, 1011, 2095, 1011, 2214, 2567, 1997, 10457, 13334, 2102, 3337, 2632, 2819, 4172, 5708, 1056, 28394, 3064, 2006, 5095, 2305, 1037, 2146, 2330, 3661, 1999, 2029, 2002, 28049, 2010, 8432, 2000, 2119, 2273, 1998, 2308, 2144, 2002, 2001, 2410, 1012, 1000, 2045, 1521, 1055, 2242, 1045, 1521, 1040, 2066, 2000, 2360, 2008, 1045, 2514, 2003, 2590, 2005, 2870, 1998, 2026, 4767, 2008, 2038, 2042, 15243, 2006, 2026, 3108, 2005, 3053, 2431, 1997, 2026, 2166, 1010, 1000, 2002, 2626, 1012, 1000, 2023, 2987, 1521, 1056, 3288, 2033, 9467, 1010, 2074, 1037, 3635, 1998, 10859, 1045, 2031, 2218, 3031, 2005, 1037, 2146, 2051, 2008, 1045, 2052, 2066, 4196, 2125, 2033, 1012, 1000, 2002, 7607, 1010, 1000, 1045, 3473, 2039, 1999, 2023, 4024, 3068, 2012, 1037, 2200, 2402, 2287, 1998, 2043, 1045, 2001, 2105, 2410, 2086, 2214, 1045, 2318, 2000, 2424, 3337, 1998, 3057, 8702, 1012, 1000, 2002, 2059, 7657, 1010, 1000, 2045, 2020, 2086, 2008, 2253, 2011, 2008, 1045, 2245, 2055, 1010, 2021, 2009, 2347, 1521, 1056, 2127, 1045, 2001, 2459, 2086, 2214, 1010, 2044, 1037, 2261, 6550, 2007, 3057, 1010, 1045, 2018, 2019, 3325, 2007, 1037, 3287, 2008, 1045, 2018, 2019, 8432, 2000, 2040, 1045, 2036, 2499, 2007, 1998, 3473, 2039, 2007, 1012, 1000, 5708, 4247, 1010, 1000, 2000, 2033, 2189, 2038, 2467, 2042, 2026, 3379, 1012, 2189, 2097, 2467, 2022, 2054, 9099, 23865, 2015, 2149, 1998, 2870, 1012, 1996, 2996, 2038, 2467, 2042, 2026, 3647, 4033, 1012, 2021, 1996, 7209, 3125, 2005, 2033, 2003, 2000, 2022, 8510, 1012, 1045, 2196, 2215, 2000, 2022, 1037, 3275, 1997, 10520, 1012, 1000, 1037, 9925, 1010, 3516, 1010, 3128, 1010, 2002, 4515, 1996, 3661, 2007, 1037, 14686, 2013, 3220, 2879, 2577, 1010, 2040, 2036, 2038, 2042, 2330, 1999, 10537, 2010, 13798, 1012, 1000, 1996, 2190, 14686, 2000, 7680, 1005, 1045, 1005, 2310, 2196, 2371, 2004, 2295, 1045, 2134, 1005, 1056, 7141, 1010, 1045, 2074, 6051, 2004, 2295, 1045, 2106, 1012, 1005, 1517, 2879, 2577, 1000, 5708, 1005, 1055, 1056, 28394, 2102, 4076, 2010, 2251, 2321, 6545, 1999, 4108, 2006, 10928, 1997, 4439, 2104, 1996, 3747, 1998, 16204, 6664, 1012, 2010, 6513, 1010, 7063, 6262, 1010, 2001, 1999, 1996, 4628, 2835, 1997, 1996, 2482, 1998, 2001, 2036, 2579, 2046, 2610, 9968, 1012, 5708, 2001, 2207, 2044, 14739, 15358, 2005, 1002, 1018, 1010, 19827, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'str'>\n",
      "[101, 3220, 7158, 5708, 1010, 2040, 2038, 3130, 2042, 16875, 2055, 2010, 9415, 6905, 1998, 5983, 8761, 1010, 2003, 2085, 3098, 2039, 2055, 2010, 13798, 1012, 1996, 2756, 1011, 2095, 1011, 2214, 2567, 1997, 10457, 13334, 2102, 3337, 2632, 2819, 4172, 5708, 1056, 28394, 3064, 2006, 5095, 2305, 1037, 2146, 2330, 3661, 1999, 2029, 2002, 28049, 2010, 8432, 2000, 2119, 2273, 1998, 2308, 2144, 2002, 2001, 2410, 1012, 1000, 2045, 1521, 1055, 2242, 1045, 1521, 1040, 2066, 2000, 2360, 2008, 1045, 2514, 2003, 2590, 2005, 2870, 1998, 2026, 4767, 2008, 2038, 2042, 15243, 2006, 2026, 3108, 2005, 3053, 2431, 1997, 2026, 2166, 1010, 1000, 2002, 2626, 1012, 1000, 2023, 2987, 1521, 1056, 3288, 2033, 9467, 1010, 2074, 1037, 3635, 1998, 10859, 1045, 2031, 2218, 3031, 2005, 1037, 2146, 2051, 2008, 1045, 2052, 2066, 4196, 2125, 2033, 1012, 1000, 2002, 7607, 1010, 1000, 1045, 3473, 2039, 1999, 2023, 4024, 3068, 2012, 1037, 2200, 2402, 2287, 1998, 2043, 1045, 2001, 2105, 2410, 2086, 2214, 1045, 2318, 2000, 2424, 3337, 1998, 3057, 8702, 1012, 1000, 2002, 2059, 7657, 1010, 1000, 2045, 2020, 2086, 2008, 2253, 2011, 2008, 1045, 2245, 2055, 1010, 2021, 2009, 2347, 1521, 1056, 2127, 1045, 2001, 2459, 2086, 2214, 1010, 2044, 1037, 2261, 6550, 2007, 3057, 1010, 1045, 2018, 2019, 3325, 2007, 1037, 3287, 2008, 1045, 2018, 2019, 8432, 2000, 2040, 1045, 2036, 2499, 2007, 1998, 3473, 2039, 2007, 1012, 1000, 5708, 4247, 1010, 1000, 2000, 2033, 2189, 2038, 2467, 2042, 2026, 3379, 1012, 2189, 2097, 2467, 2022, 2054, 9099, 23865, 2015, 2149, 1998, 2870, 1012, 1996, 2996, 2038, 2467, 2042, 2026, 3647, 4033, 1012, 2021, 1996, 7209, 3125, 2005, 2033, 2003, 2000, 2022, 8510, 1012, 1045, 2196, 2215, 2000, 2022, 1037, 3275, 1997, 10520, 1012, 1000, 1037, 9925, 1010, 3516, 1010, 3128, 1010, 2002, 4515, 1996, 3661, 2007, 1037, 14686, 2013, 3220, 2879, 2577, 1010, 2040, 2036, 2038, 2042, 2330, 1999, 10537, 2010, 13798, 1012, 1000, 1996, 2190, 14686, 2000, 7680, 1005, 1045, 1005, 2310, 2196, 2371, 2004, 2295, 1045, 2134, 1005, 1056, 7141, 1010, 1045, 2074, 6051, 2004, 2295, 1045, 2106, 1012, 1005, 1517, 2879, 2577, 1000, 5708, 1005, 1055, 1056, 28394, 2102, 4076, 2010, 2251, 2321, 6545, 1999, 4108, 2006, 10928, 1997, 4439, 2104, 1996, 3747, 1998, 16204, 6664, 1012, 2010, 6513, 1010, 7063, 6262, 1010, 2001, 1999, 1996, 4628, 2835, 1997, 1996, 2482, 1998, 2001, 2036, 2579, 2046, 2610, 9968, 1012, 5708, 2001, 2207, 2044, 14739, 15358, 2005, 1002, 1018, 1010, 19827, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# transfrom embedding to list of int\n",
    "import ast\n",
    "\n",
    "# read\n",
    "print(train_df['embedding'][0])   \n",
    "print(type(train_df['embedding'][0])) # string\n",
    "\n",
    "# convert the embeddings to list\n",
    "train_df['embedding'] = train_df['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "val_df['embedding'] = val_df['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "test_df['embedding'] = test_df['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# convert the embeddings to list of integers\n",
    "train_df['embedding'] = train_df['embedding'].apply(lambda x: list(map(int, x)))\n",
    "val_df['embedding'] = val_df['embedding'].apply(lambda x: list(map(int, x)))\n",
    "test_df['embedding'] = test_df['embedding'].apply(lambda x: list(map(int, x)))\n",
    "\n",
    "print(train_df['embedding'][0])\n",
    "print(type(train_df['embedding'][0])) # list of integers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3220, 7158, 5708, 1010, 2040, 2038, 3130, 2042, 16875, 2055, 2010, 9415, 6905, 1998, 5983, 8761, 1010, 2003, 2085, 3098, 2039, 2055, 2010, 13798, 1012, 1996, 2756, 1011, 2095, 1011, 2214, 2567, 1997, 10457, 13334, 2102, 3337, 2632, 2819, 4172, 5708, 1056, 28394, 3064, 2006, 5095, 2305, 1037, 2146, 2330, 3661, 1999, 2029, 2002, 28049, 2010, 8432, 2000, 2119, 2273, 1998, 2308, 2144, 2002, 2001, 2410, 1012, 1000, 2045, 1521, 1055, 2242, 1045, 1521, 1040, 2066, 2000, 2360, 2008, 1045, 2514, 2003, 2590, 2005, 2870, 1998, 2026, 4767, 2008, 2038, 2042, 15243, 2006, 2026, 3108, 2005, 3053, 2431, 1997, 2026, 2166, 1010, 1000, 2002, 2626, 1012, 1000, 2023, 2987, 1521, 1056, 3288, 2033, 9467, 1010, 2074, 1037, 3635, 1998, 10859, 1045, 2031, 2218, 3031, 2005, 1037, 2146, 2051, 2008, 1045, 2052, 2066, 4196, 2125, 2033, 1012, 1000, 2002, 7607, 1010, 1000, 1045, 3473, 2039, 1999, 2023, 4024, 3068, 2012, 1037, 2200, 2402, 2287, 1998, 2043, 1045, 2001, 2105, 2410, 2086, 2214, 1045, 2318, 2000, 2424, 3337, 1998, 3057, 8702, 1012, 1000, 2002, 2059, 7657, 1010, 1000, 2045, 2020, 2086, 2008, 2253, 2011, 2008, 1045, 2245, 2055, 1010, 2021, 2009, 2347, 1521, 1056, 2127, 1045, 2001, 2459, 2086, 2214, 1010, 2044, 1037, 2261, 6550, 2007, 3057, 1010, 1045, 2018, 2019, 3325, 2007, 1037, 3287, 2008, 1045, 2018, 2019, 8432, 2000, 2040, 1045, 2036, 2499, 2007, 1998, 3473, 2039, 2007, 1012, 1000, 5708, 4247, 1010, 1000, 2000, 2033, 2189, 2038, 2467, 2042, 2026, 3379, 1012, 2189, 2097, 2467, 2022, 2054, 9099, 23865, 2015, 2149, 1998, 2870, 1012, 1996, 2996, 2038, 2467, 2042, 2026, 3647, 4033, 1012, 2021, 1996, 7209, 3125, 2005, 2033, 2003, 2000, 2022, 8510, 1012, 1045, 2196, 2215, 2000, 2022, 1037, 3275, 1997, 10520, 1012, 1000, 1037, 9925, 1010, 3516, 1010, 3128, 1010, 2002, 4515, 1996, 3661, 2007, 1037, 14686, 2013, 3220, 2879, 2577, 1010, 2040, 2036, 2038, 2042, 2330, 1999, 10537, 2010, 13798, 1012, 1000, 1996, 2190, 14686, 2000, 7680, 1005, 1045, 1005, 2310, 2196, 2371, 2004, 2295, 1045, 2134, 1005, 1056, 7141, 1010, 1045, 2074, 6051, 2004, 2295, 1045, 2106, 1012, 1005, 1517, 2879, 2577, 1000, 5708, 1005, 1055, 1056, 28394, 2102, 4076, 2010, 2251, 2321, 6545, 1999, 4108, 2006, 10928, 1997, 4439, 2104, 1996, 3747, 1998, 16204, 6664, 1012, 2010, 6513, 1010, 7063, 6262, 1010, 2001, 1999, 1996, 4628, 2835, 1997, 1996, 2482, 1998, 2001, 2036, 2579, 2046, 2610, 9968, 1012, 5708, 2001, 2207, 2044, 14739, 15358, 2005, 1002, 1018, 1010, 19827, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n",
      "3988\n",
      "[101, 2577, 10805, 18856, 7828, 3240, 1006, 2141, 2089, 1020, 1010, 3777, 1007, 2003, 2019, 2137, 3364, 1010, 2472, 1010, 3135, 1010, 11167, 1010, 1998, 6883, 1012, 2002, 2038, 2363, 2093, 3585, 7595, 2982, 2005, 2010, 2147, 2004, 2019, 3364, 1998, 2048, 2914, 2982, 1010, 2028, 2005, 3772, 1999, 9042, 2050, 1006, 2294, 1007, 1998, 1996, 2060, 2005, 2522, 1011, 5155, 12098, 3995, 1006, 2262, 1007, 1012, 18856, 7828, 3240, 2081, 2010, 3772, 2834, 2006, 2547, 1999, 3301, 1010, 1998, 2101, 4227, 2898, 5038, 1999, 2010, 2535, 2004, 2852, 1012, 8788, 5811, 2006, 1996, 2146, 1011, 2770, 2966, 3689, 9413, 2013, 2807, 2000, 2639, 1010, 2005, 2029, 2002, 2363, 2048, 18474, 10096, 2400, 9930, 1012, 2096, 2551, 2006, 9413, 1010, 2002, 2211, 15411, 1037, 3528, 1997, 2877, 4395, 1999, 3152, 1010, 2164, 1996, 16251, 2143, 8942, 1004, 5863, 1006, 2722, 1007, 1998, 1996, 4126, 4038, 2041, 1997, 4356, 1006, 2687, 1007, 1010, 1999, 2029, 2002, 2034, 2499, 2007, 2472, 7112, 2061, 4063, 4059, 2232, 1010, 2040, 2052, 2468, 1037, 2146, 1011, 2051, 18843, 1012, 1999, 2639, 1010, 2002, 2165, 1996, 2599, 2535, 1999, 2093, 5465, 1010, 1037, 2092, 1011, 2363, 2162, 18312, 2275, 2076, 1996, 6084, 2162, 1012, 1999, 2541, 1010, 18856, 7828, 3240, 1005, 1055, 4476, 8723, 2007, 1996, 2713, 1997, 2010, 5221, 3293, 3112, 1010, 1996, 2002, 2923, 4038, 12661, 4153, 1005, 1055, 5408, 1010, 1996, 2034, 1997, 2054, 2150, 1037, 11544, 4626, 18856, 7828, 3240, 1012, 2002, 2081, 2010, 21635, 2834, 1037, 2095, 2101, 2007, 1996, 16747, 8645, 4038, 21444, 1997, 1037, 4795, 2568, 1010, 1998, 2038, 2144, 2856, 1996, 3439, 3689, 2204, 2305, 1010, 1998, 2204, 6735, 1006, 2384, 1007, 1010, 1996, 2998, 4038, 5898, 13038, 1006, 2263, 1007, 1010, 1996, 2576, 3689, 1996, 8909, 2229, 1997, 2233, 1006, 2249, 1007, 1010, 1998, 1996, 2162, 2143, 1996, 10490, 2273, 1006, 2297, 1007, 1012, 18856, 7828, 3240, 2180, 2019, 2914, 2400, 2005, 2190, 4637, 3364, 2005, 1996, 2690, 2264, 10874, 9042, 2050, 1006, 2384, 1007, 1010, 1998, 3525, 3687, 2190, 3364, 9930, 2005, 1996, 3423, 10874, 2745, 11811, 1006, 2289, 1007, 1998, 1996, 4038, 1011, 16547, 2039, 1999, 1996, 2250, 1006, 2268, 1007, 1998, 1996, 8481, 1006, 2249, 1007, 1012, 1999, 2286, 1010, 2002, 2363, 1996, 2914, 2400, 2005, 2190, 3861, 2005, 5155, 1996, 2576, 10874, 12098, 3995, 1012, 2002, 2003, 1996, 2069, 2711, 2040, 2038, 2042, 4222, 2005, 2914, 2982, 1999, 2416, 2367, 7236, 1012, 1031, 1017, 1033, 1999, 2268, 1010, 18856, 7828, 3240, 2001, 2443, 1999, 2051, 1005, 1055, 3296, 2051, 2531, 2004, 2028, 1997, 1996, 1000, 2087, 6383, 2111, 1999, 1996, 2088, 1000, 1012, 1031, 1018, 1033, 2002, 2003, 2036, 3264, 2005, 2010, 2576, 1998, 3171, 16841, 1010, 1998, 2038, 2366, 2004, 2028, 1997, 1996, 2142, 3741, 28938, 1997, 3521, 2144, 2254, 2861, 1010, 2263, 1012, 1031, 1019, 1033, 1031, 1020, 1033, 1031, 1021, 1033, 2010, 11470, 2147, 2950, 2010, 12288, 1997, 4531, 1037, 5813, 2005, 1996, 18243, 27942, 4736, 1010, 6274, 5029, 2005, 1996, 2230, 12867, 8372, 1010, 7508, 14052, 5038, 1010, 2432, 19267, 1010, 1023, 1013, 2340, 5694, 1010, 1998, 4526, 15693, 2107, 102]\n",
      "<class 'list'>\n",
      "998\n",
      "[101, 1996, 2418, 9458, 3601, 2982, 5103, 2001, 2218, 2006, 2257, 2410, 1010, 2418, 1012, 1031, 1015, 1033, 1996, 2982, 6334, 1996, 2095, 1005, 1055, 10106, 1999, 2189, 1010, 2143, 1010, 2547, 1010, 2998, 1010, 4827, 1010, 4038, 1010, 1998, 1996, 4274, 1010, 1998, 2020, 5444, 2006, 2011, 7193, 2542, 1999, 1996, 3915, 1010, 4793, 2410, 1998, 2058, 2083, 2536, 2591, 2865, 4573, 1012, 1031, 1016, 1033, 1037, 2093, 3178, 3315, 2782, 2170, 1000, 9458, 17037, 1000, 1998, 4354, 2011, 5180, 2703, 2001, 18498, 7580, 2006, 7858, 2007, 2070, 1997, 1996, 2724, 6037, 2076, 1996, 9458, 3601, 3743, 1012, 1031, 1017, 1033, 22222, 1019, 2363, 1996, 7725, 5476, 2400, 1012, 1031, 1018, 1033, 2802, 1996, 2265, 1010, 2195, 12330, 1010, 2164, 13226, 15876, 11818, 3619, 1010, 16729, 10259, 2050, 1998, 10294, 14855, 5397, 25698, 8280, 1996, 10530, 1997, 1996, 2418, 15908, 1996, 2157, 8320, 1998, 6628, 13496, 2000, 3713, 2041, 2114, 4808, 1998, 5223, 1012, 2023, 2003, 1996, 2034, 5103, 2144, 2526, 2000, 2025, 2421, 1037, 3677, 1012, 9567, 1031, 10086, 1033, 25588, 1031, 10086, 1033, 4791, 1998, 17853, 1031, 10086, 1033, 1996, 2034, 4400, 1997, 9930, 2020, 2623, 2006, 2238, 2539, 1010, 2418, 1012, 1031, 1022, 1033, 1996, 2117, 4400, 2001, 2623, 2006, 2251, 2260, 1010, 2418, 1012, 1031, 1023, 1033, 4791, 2024, 3205, 2034, 1010, 1999, 7782, 1012, 1031, 2184, 1033, 5691, 1031, 10086, 1033, 2547, 1031, 10086, 1033, 5691, 1004, 2547, 1031, 10086, 1033, 2189, 1031, 10086, 1033, 3617, 1031, 10086, 1033, 4827, 1031, 10086, 1033, 2998, 1031, 10086, 1033, 25408, 1031, 10086, 1033, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n",
      "1247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_df['embedding'][0])\n",
    "print(type(train_df['embedding'][0])) # list of integers\n",
    "print(len(train_df))\n",
    "\n",
    "print(val_df['embedding'][0])\n",
    "print(type(val_df['embedding'][0])) # list of integers\n",
    "print(len(val_df))\n",
    "\n",
    "print(test_df['embedding'][0])\n",
    "print(type(test_df['embedding'][0])) # list of integers\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install package for PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "%pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "%pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "%pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "%pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['embedding', 'label'], dtype='object')\n",
      "Index(['embedding', 'label'], dtype='object')\n",
      "Index(['id', 'embedding'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "print(train_df.columns)\n",
    "print(val_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caculate Cosine Similarity for training embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to PyTorch tensors\n",
    "train_df['embedding'] = train_df['embedding'].apply(lambda x: torch.tensor(x))\n",
    "\n",
    "# Assuming train_df is already defined and has 'embedding' column with PyTorch tensors\n",
    "embeddings = torch.stack(train_df['embedding'].tolist())  # Convert list of tensors to a tensor\n",
    "\n",
    "# Convert tensor embedding to float\n",
    "embeddings = embeddings.float()\n",
    "\n",
    "print(f\"Shape of the embeddings tensor: {embeddings.shape}\")\n",
    "\n",
    "# Normalize the embeddings to have unit length\n",
    "norm_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = torch.mm(norm_embeddings, norm_embeddings.t())\n",
    "\n",
    "print(f\"Shape of the cosine similarity DataFrame: {cosine_sim.shape}\")\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Adjencency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the upper triangle of the cosine similarity matrix\n",
    "cosine_sim_scalar = cosine_sim.flatten()\n",
    "# print(cosine_sim_scalar)\n",
    "cosine_sim_scalar = cosine_sim_scalar[cosine_sim_scalar != 1]  # Remove the diagonal elements\n",
    "# print(cosine_sim_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 1.0000009536743164\n",
      "Min: 0.009328627958893776\n",
      "Mean: 0.3206745982170105\n",
      "Median: 0.34415799379348755\n",
      "Standard Deviation: 0.09867545962333679\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Median of the cosine similarity matrix\n",
    "\n",
    "stats_max = torch.max(cosine_sim_scalar).item()\n",
    "stats_min = torch.min(cosine_sim_scalar).item()\n",
    "stats_mean = torch.mean(cosine_sim_scalar).item()\n",
    "stats_median = torch.median(cosine_sim_scalar).item()\n",
    "stats_std = torch.std(cosine_sim_scalar).item()\n",
    "\n",
    "print(f\"Max: {stats_max}\")\n",
    "print(f\"Min: {stats_min}\")\n",
    "print(f\"Mean: {stats_mean}\")\n",
    "print(f\"Median: {stats_median}\")\n",
    "print(f\"Standard Deviation: {stats_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(cosine_sim.numpy())\n",
    "\n",
    "# for each row, if the value bigger than Median, then 1, else 0\n",
    "A = (A > stats_median).astype(int)\n",
    "\n",
    "# and make sure diagonal to be 0\n",
    "np.fill_diagonal(A.values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjency Matrix: \n",
      "      0     1     2     3     4     5     6     7     8     9     ...  3978  \\\n",
      "0        0     1     0     1     0     1     0     1     1     1  ...     1   \n",
      "1        1     0     0     1     0     1     1     1     1     0  ...     1   \n",
      "2        0     0     0     0     1     0     0     1     0     1  ...     1   \n",
      "3        1     1     0     0     0     1     1     1     1     1  ...     1   \n",
      "4        0     0     1     0     0     1     0     0     0     0  ...     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
      "3983     1     1     0     1     0     1     1     1     1     1  ...     1   \n",
      "3984     1     1     0     1     0     1     1     1     1     1  ...     1   \n",
      "3985     0     1     0     1     1     1     1     1     1     1  ...     1   \n",
      "3986     0     1     1     1     0     0     0     1     1     0  ...     1   \n",
      "3987     0     1     1     0     0     0     1     1     0     1  ...     1   \n",
      "\n",
      "      3979  3980  3981  3982  3983  3984  3985  3986  3987  \n",
      "0        1     1     0     1     1     1     0     0     0  \n",
      "1        1     1     0     1     1     1     1     1     1  \n",
      "2        0     0     0     1     0     0     0     1     1  \n",
      "3        1     1     0     1     1     1     1     1     0  \n",
      "4        0     0     0     1     0     0     1     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "3983     1     1     0     1     0     1     1     1     0  \n",
      "3984     1     1     0     1     1     0     1     1     1  \n",
      "3985     1     1     0     1     1     1     0     1     0  \n",
      "3986     1     1     0     1     1     1     1     0     0  \n",
      "3987     0     0     0     1     0     1     0     0     0  \n",
      "\n",
      "[3988 rows x 3988 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Adjency Matrix: \")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 1, 1],\n",
      "        ...,\n",
      "        [0, 1, 0,  ..., 0, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 0, 0],\n",
      "        [0, 1, 1,  ..., 0, 0, 0]])\n",
      "torch.Size([3988, 3988])\n",
      "<class 'torch.Tensor'>\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "A_list = torch.tensor(A.values)\n",
    "print(A_list)\n",
    "print(A_list.shape)\n",
    "print(type(A_list))\n",
    "print(A_list.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  3220  7158 ...     0     0     0]\n",
      " [  101 11977  2086 ...  2265  2018   102]\n",
      " [  101  1996  2388 ...     0     0     0]\n",
      " ...\n",
      " [  101  1996  5119 ...  9826 17540   102]\n",
      " [  101  2909  2520 ...  2004 19592   102]\n",
      " [  101  7986  2019 ...     0     0     0]]\n",
      "(3988,)\n"
     ]
    }
   ],
   "source": [
    "# print(train_df['embedding'])\n",
    "print(np.array(train_df['embedding'].tolist()))\n",
    "print(train_df['embedding'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df):\n",
    "    X = torch.tensor(np.array(df['embedding'].tolist()), dtype=torch.float32)\n",
    "    Y = torch.tensor(np.array(df['label'].tolist()), dtype=torch.int64)\n",
    "    \n",
    "    edge_index = A_list\n",
    "    \n",
    "    return Data(x=X, edge_index=edge_index, y=Y)\n",
    "\n",
    "train_data = create_data(train_df)\n",
    "val_data = create_data(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Data(x=[3988, 512], edge_index=[3988, 3988], y=[3988])\n",
      "val: Data(x=[998, 512], edge_index=[3988, 3988], y=[998])\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {train_data}\")\n",
    "print(f\"val: {val_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "def build_graph(data):\n",
    "    '''\n",
    "    data: PyTorch Geometric Data object\n",
    "    - x: Node features\n",
    "    - edge_index: Adjenct matrix [x.size(0), x.size(0)]\n",
    "    - y: Node labels\n",
    "    '''\n",
    "    x, edge_index, y = data.x, data.edge_index, data.y\n",
    "    \n",
    "\n",
    "    # initialize a undirected graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "\n",
    "    # add nodes\n",
    "    for i in range(x.size(0)):\n",
    "        G.add_node(i, label=y[i].item())\n",
    "\n",
    "    # convert adjency matrix to COO format\n",
    "    coo = coo_matrix(edge_index.numpy())\n",
    "\n",
    "    # add edges\n",
    "    for i, j in zip(coo.row, coo.col):\n",
    "        G.add_edge(i, j)\n",
    "\n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_G = build_graph(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the figure is too big, cost time\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# nx.draw(train_G, with_labels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import tqdm\n",
    "\n",
    "# def graph_describe(G):\n",
    "#     # print(\"Number of nodes:\",G.number_of_nodes())\n",
    "#     # print(\"Number of edges:\",G.number_of_edges())\n",
    "#     density=2*G.number_of_edges()/(G.number_of_nodes()*(G.number_of_nodes()-1))\n",
    "#     # print(f\"density = {round(density,round_digit)}\")\n",
    "#     # print(\"Degree:\")\n",
    "#     # 获取节点的度数\n",
    "#     degrees = [degree for node, degree in G.degree()]\n",
    "#     # 计算统计指标\n",
    "#     mean_degree = np.mean(degrees)\n",
    "#     median_degree = np.median(degrees)\n",
    "#     std_degree = np.std(degrees)\n",
    "#     max_degree = np.max(degrees)\n",
    "#     min_degree = np.min(degrees)\n",
    "#     # # 打印结果\n",
    "#     # print(\"Mean Degree:\", round(mean_degree,round_digit))\n",
    "#     # print(\"Median Degree:\", round(median_degree,round_digit))\n",
    "#     # print(\"Standard Deviation of Degree:\", round(std_degree,round_digit))\n",
    "#     # print(\"Max Degree:\", round(max_degree,round_digit))\n",
    "#     # print(\"Min Degree:\", round(min_degree,round_digit))\n",
    "\n",
    "#     edge_consistency={}\n",
    "#     fraud_node_set=set()\n",
    "#     for edge in tqdm(G.edges(),desc=\"Check edge consistent...\"):\n",
    "#         node1_class=G.nodes[edge[0]]['label']\n",
    "#         node2_class=G.nodes[edge[1]]['label']\n",
    "\n",
    "#         if node1_class==node2_class:\n",
    "#             edge_consistency[edge]=1\n",
    "#         else:\n",
    "#             edge_consistency[edge]=0\n",
    "\n",
    "#     value_counts=Counter(edge_consistency.values())\n",
    "#     # print(value_counts)\n",
    "#     ratio=value_counts[0]/G.number_of_edges()\n",
    "#     # print(f\"The ratio of heterogeneous edges is:{round(ratio,round_digit)}\")\n",
    "#     print(\"Graph Descriptive Analysis:\")\n",
    "#     print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "#     print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "#     print(f\"Density: {round(density, 4)}\")\n",
    "#     print(f\"Mean Degree: {round(mean_degree, 4)}\")\n",
    "#     print(f\"Median Degree: {round(median_degree, 4)}\")\n",
    "#     print(f\"Standard Deviation of Degree: {round(std_degree, 4)}\")\n",
    "#     print(f\"Max Degree: {round(max_degree, 4)}\")\n",
    "#     print(f\"Min Degree: {round(min_degree, 4)}\")\n",
    "#     print(f\"The ratio of heterogeneous edges is: {round(ratio, 4)}\")\n",
    "    \n",
    "#     return G.number_of_nodes(),G.number_of_edges(),density,mean_degree,median_degree,std_degree,max_degree,min_degree,ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n",
      "3974233\n"
     ]
    }
   ],
   "source": [
    "print(train_G.number_of_nodes())\n",
    "print(train_G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 模型参数\n",
    "input_dim = train_data.num_node_features\n",
    "hidden_dim = 16\n",
    "output_dim = len(train_df['label'].unique())\n",
    "\n",
    "# 创建模型实例\n",
    "model = GCN(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data)\n",
    "    loss = F.nll_loss(out, train_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train()\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data):\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = pred.eq(data.y).sum().item()\n",
    "    accuracy = correct / len(data.y)\n",
    "    return accuracy\n",
    "\n",
    "train_acc = test(train_data)\n",
    "val_acc = test(val_data)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc * 100:.2f}%')\n",
    "print(f'Validation Accuracy: {val_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
