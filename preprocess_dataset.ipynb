{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prprocess Dataset: text to embedding\n",
    "- ref: https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, BertModel, BertTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv('dataset/train.csv', sep='\\t', encoding='utf-8')\n",
    "test_df = pd.read_csv('dataset/test.csv', sep='\\t', encoding='utf-8')\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "print(f\"Testing data shape: {test_df.shape}\")\n",
    "print(test_df.head())   # no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n",
    "# check NaN values\n",
    "print(f\"Null values in training data:\")\n",
    "print(train_df.isnull().sum())\n",
    "# print unique labels\n",
    "print(f\"Unique labels in training data:\")\n",
    "print(train_df['label'].unique())\n",
    "# find the row that label == 'label'\n",
    "print(f\"Rows with label 'label':\")\n",
    "print(train_df[train_df['label'] == 'label'])\n",
    "\n",
    "# remove the row that label == 'label'\n",
    "train_df = train_df[train_df['label'] != 'label']\n",
    "\n",
    "# save labels as int type\n",
    "train_df['label'] = train_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# info\n",
    "print(f\"train info:\")\n",
    "print(train_df.info())\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test info:\")\n",
    "print(test_df.info())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation split\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "\n",
    "print(train_df.head())\n",
    "print(val_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "At this stage, we transform the text data into embeddings to later feed into the model. \n",
    "\n",
    "We choose the `distilbert-base-uncased` model on Hugging Face for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "train_encodings = tokenizer(train_df['text'].tolist(), max_length=512, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_df['text'].tolist(), max_length=512, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_df['text'].tolist(), max_length=512, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After tokenization, the texts are converted to input IDs and attention masks\n",
    "print(train_encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_encodings['input_ids'][0])\n",
    "\n",
    "print(train_encodings['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# move the model to the device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.encodings['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encodings['attention_mask'][idx])\n",
    "        labels = self.labels[idx]\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset objects\n",
    "train_dataset = TokenDataset(train_encodings, train_df['label'].tolist())\n",
    "val_dataset = TokenDataset(val_encodings, val_df['label'].tolist())\n",
    "test_dataset = TokenDataset(test_encodings, np.zeros(test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodings_to_embeddings(loader, model):\n",
    "    model.eval()\n",
    "    df = pd.DataFrame()\n",
    "    embeddings_record = []\n",
    "    labels_record = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in tqdm(loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            embeddings = last_hidden_states\n",
    "            \n",
    "            embeddings = torch.mean(embeddings, dim=1)\n",
    "            \n",
    "            embeddings_record.extend(embeddings.cpu().numpy())\n",
    "            labels_record.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # print(embeddings.shape)\n",
    "            # print(labels.shape)\n",
    "            \n",
    "            # print(len(embeddings_record))\n",
    "            # print(len(labels_record))\n",
    "            \n",
    "    df['embeddings'] = embeddings_record\n",
    "    df['labels'] = labels_record\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_df = encodings_to_embeddings(train_loader, model)\n",
    "val_embeddings_df = encodings_to_embeddings(val_loader, model)\n",
    "test_embeddings_df = encodings_to_embeddings(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out embedding to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embeddings_df.head())\n",
    "print(val_embeddings_df.head())\n",
    "print(test_embeddings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframes to csv files\n",
    "train_embeddings_df.to_csv('dataset/train_embeddings.csv', sep='\\t', index=False)\n",
    "val_embeddings_df.to_csv('dataset/val_embeddings.csv', sep='\\t', index=False)\n",
    "test_embeddings_df.to_csv('dataset/test_embeddings.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# read\n",
    "sample = pd.read_csv('dataset/train_embeddings.csv', sep='\\t', encoding='utf-8')\n",
    "print(sample.head())\n",
    "\n",
    "print(sample['embeddings'][0])   \n",
    "print(type(sample['embeddings'][0])) # string\n",
    "\n",
    "# Function to convert the string representation of the array to a list of floats\n",
    "def convert_str_to_float_list(s):\n",
    "    s = s.strip('[]')\n",
    "    # Split the string on spaces, filter out any empty strings that result from consecutive spaces\n",
    "    number_strings = filter(None, s.split(' '))\n",
    "    # Convert each string to a float and return the list\n",
    "    return [float(x) for x in number_strings]\n",
    "\n",
    "# convert the embeddings to list of floats\n",
    "sample['embeddings'] = sample['embeddings'].apply(convert_str_to_float_list)\n",
    "\n",
    "print(sample['embeddings'][0])\n",
    "print(type(sample['embeddings'][0])) # list of integers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forgery-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
